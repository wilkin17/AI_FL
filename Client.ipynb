{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Read method and number of rounds from config.txt\n",
    "if os.path.exists(\"config.txt\"):\n",
    "    with open(\"config.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        method = lines[0].strip()\n",
    "        num_rounds = int(lines[1].strip())\n",
    "        dataset = lines[2].strip()\n",
    "        seed = lines[3].strip()\n",
    "else:\n",
    "    raise FileNotFoundError(\"Config file not found. Please run the server first.\")\n",
    "\n",
    "if dataset == \"Noisy\":\n",
    "    train_df = pd.read_csv(r\".\\trainWNoise.csv\")\n",
    "    test_df = pd.read_csv(r\".\\testWNoise.csv\")\n",
    "else:\n",
    "    train_df = pd.read_csv(r\".\\Train_Aggregated.csv\")\n",
    "    test_df = pd.read_csv(r\".\\Test_Aggregated.csv\")\n",
    "\n",
    "# Parameters\n",
    "data_per_client = 247  # Training data points per client.\n",
    "test_data_points = 102  # Number of test data points.\n",
    "sequence_length = 50\n",
    "prediction_length = 50\n",
    "num_iterations = 10\n",
    "\n",
    "# Initialize storage for results\n",
    "client_loss_data_all = {}\n",
    "first_seed = seed\n",
    "\n",
    "# Calculate the number of clients\n",
    "train_data_size = len(train_df)\n",
    "num_clients = train_data_size // data_per_client\n",
    "\n",
    "# Verify that we have enough data\n",
    "total_required_data = num_clients * data_per_client\n",
    "if train_data_size < total_required_data:\n",
    "    raise ValueError(f\"Not enough training data. Required: {total_required_data}, Available: {train_data_size}\")\n",
    "\n",
    "# Ask the user if they want to change the method\n",
    "default_method = method # Set the default method\n",
    "method_map = {\"0\": \"FedAvg\", \"1\": \"FedDyn\", \"2\": \"IDA\", \"3\": \"Robust\", \"4\": \"HyFDCA\"}\n",
    "method_input = input(f\"Enter the training method (0: FedAvg, 1: FedDyn, 2: IDA, 3: Robust, 4: HyFDCA) or press Enter to keep [{default_method}]: \").strip()\n",
    "\n",
    "# If the user provided a method, update it; otherwise, keep the default\n",
    "if method_input in method_map:\n",
    "    method = method_map[method_input]\n",
    "else:\n",
    "    method = default_method\n",
    "print(f\"Using method: {method}\")  # Confirm the method selection\n",
    "    \n",
    "random.seed(seed) \n",
    "\n",
    "device_evaluation_df = pd.DataFrame(columns=[\"client_id\", \"loss\"])\n",
    "client_loss_data = {}\n",
    "\n",
    "def dynamic_weight_decay(loss, base_decay=0.01, min_decay=0.0001, max_decay=0.1):\n",
    "    \"\"\"\n",
    "    Adjust weight decay based on the loss.\n",
    "    Higher loss leads to higher regularization (to prevent overfitting).\n",
    "    Lower loss leads to lower regularization (to allow better fitting).\n",
    "    \n",
    "    Args:\n",
    "        loss (float): Current loss value.\n",
    "        base_decay (float): Base weight decay value.\n",
    "        min_decay (float): Minimum weight decay allowed.\n",
    "        max_decay (float): Maximum weight decay allowed.\n",
    "        \n",
    "    Returns:\n",
    "        float: Adjusted weight decay value.\n",
    "    \"\"\"\n",
    "    # Adjust weight decay inversely with the loss (clipped to min/max bounds)\n",
    "    decay = base_decay * (1 + loss)\n",
    "    decay = np.clip(decay, min_decay, max_decay)\n",
    "    return decay\n",
    "\n",
    "def inverse_distance_aggregation(loss, base_decay=0.01, min_decay=0.0001, max_decay=0.1):\n",
    "    \"\"\"\n",
    "    Adjust weight decay based on the loss using Inverse Distance Aggregation (IDA) and INTRAC.\n",
    "    \n",
    "    Args:\n",
    "        loss (float): Current loss value.\n",
    "        base_decay (float): Base weight decay value.\n",
    "        min_decay (float): Minimum weight decay allowed.\n",
    "        max_decay (float): Maximum weight decay allowed.\n",
    "        \n",
    "    Returns:\n",
    "        float: Adjusted weight decay value.\n",
    "    \"\"\"\n",
    "    alpha_k = 1 / (abs(loss) + 1e-8)  # Inverse distance metric\n",
    "    alpha_k = np.clip(alpha_k, 1e-3, 10)  # Clamping values for stability\n",
    "    decay = base_decay * alpha_k\n",
    "    decay = np.clip(decay, min_decay, max_decay)\n",
    "    return decay\n",
    "\n",
    "def one_step_weiszfeld(losses, base_decay=0.01, min_decay=0.0001, max_decay=0.1):\n",
    "    \"\"\"\n",
    "    Adjust weight decay using a modified Weiszfeld algorithm.\n",
    "\n",
    "    Args:\n",
    "        losses (list or np.array): List of client loss values.\n",
    "        base_decay (float): Base weight decay value.\n",
    "        min_decay (float): Minimum weight decay allowed.\n",
    "        max_decay (float): Maximum weight decay allowed.\n",
    "\n",
    "    Returns:\n",
    "        float: Adjusted weight decay value.\n",
    "    \"\"\"\n",
    "    losses = np.array(losses)\n",
    "    \n",
    "    # Compute the simple arithmetic mean\n",
    "    mean_loss = np.mean(losses)\n",
    "    \n",
    "    # Compute the weighted consensus point using one-step Weiszfeld iteration\n",
    "    weights = 1 / (np.abs(losses) + 1e-8)  # Avoid division by zero\n",
    "    weights /= np.sum(weights)  # Normalize weights\n",
    "    consensus_point = np.sum(weights * losses)\n",
    "\n",
    "    # Compute decay as Euclidean distance between consensus point and arithmetic mean\n",
    "    decay = np.abs(consensus_point - mean_loss)\n",
    "    \n",
    "    # Scale the decay using the base decay factor\n",
    "    decay = base_decay * decay\n",
    "    decay = np.clip(decay, min_decay, max_decay)\n",
    "\n",
    "    return decay\n",
    "\n",
    "# Custom Dataset for Time Series Data\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length=sequence_length, prediction_length=prediction_length):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_length = prediction_length\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.X, self.y = self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        data_array = self.data.values\n",
    "        X, y = [], []\n",
    "        for i in range(len(data_array) - self.sequence_length - self.prediction_length + 1):\n",
    "            X.append(data_array[i:i + self.sequence_length])\n",
    "            y.append(\n",
    "                data_array[\n",
    "                    i + self.sequence_length : i + self.sequence_length + self.prediction_length, -1\n",
    "                ]\n",
    "            )  # Assuming target is last column\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        return X, y\n",
    "\n",
    "# LSTM Model Definition\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, prediction_length):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.prediction_length = prediction_length\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers, batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, prediction_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size\n",
    "        ).to(x.device)\n",
    "        c0 = torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size\n",
    "        ).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :]  # Take the last output\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Federated Learning Client\n",
    "class FLClient(fl.client.NumPyClient):\n",
    "    def __init__(self, client_data):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_length = prediction_length\n",
    "        self.input_size = client_data.shape[1]  # Number of features\n",
    "        self.hidden_size = 64\n",
    "        self.num_layers = 2\n",
    "\n",
    "        # Initialize the model\n",
    "        self.model = LSTMModel(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            prediction_length=self.prediction_length,\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Prepare datasets and dataloaders\n",
    "        self.train_dataset = TimeSeriesDataset(client_data, self.sequence_length, self.prediction_length)\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        # Define loss function (MSE)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.base_weight_decay = 0.01  # Base value for weight decay\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def secure_inner_product(self, model_weights):\n",
    "                \"\"\"\n",
    "                Computes a secure encrypted inner product for the locally available features.\n",
    "                Only sends encrypted partial results to the server. Used in Hybrid Federated \n",
    "                Dual Coordinate Ascent (HyFDCA) method.\n",
    "                \"\"\"\n",
    "                encrypted_results = []\n",
    "                \n",
    "                for X_batch, _ in self.train_loader:\n",
    "                    X_batch = X_batch.to(self.device)\n",
    "                    \n",
    "                    # Compute partial dot product of available features\n",
    "                    local_inner_product = torch.matmul(X_batch, model_weights[:self.input_size])\n",
    "\n",
    "                    # Encrypt each value in the inner product\n",
    "                    encrypted_batch = [self.public_key.encrypt(float(value)) for value in local_inner_product.cpu().detach().numpy()]\n",
    "                    \n",
    "                    encrypted_results.append(encrypted_batch)\n",
    "\n",
    "                return encrypted_results\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = {k: torch.tensor(np.copy(v), dtype=torch.float32) for k, v in params_dict}\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        self.model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Compute secure inner product before training\n",
    "        if method == \"HyFDCA\":\n",
    "            encrypted_inner_product = self.secure_inner_product(parameters)\n",
    "\n",
    "        # Adjust weight decay based on the current performance\n",
    "        if method == \"FedDyn\" or method == \"HyFDCA\":\n",
    "            current_weight_decay = dynamic_weight_decay(epoch_loss)\n",
    "        elif method == \"IDA\":\n",
    "            current_weight_decay = inverse_distance_aggregation(epoch_loss)\n",
    "        elif method == \"Robust\":\n",
    "            current_weight_decay = one_step_weiszfeld(epoch_loss)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001, weight_decay=current_weight_decay)\n",
    "\n",
    "        for epoch in range(1):  # Single epoch per round\n",
    "            for X_batch, y_batch in self.train_loader:\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                y_batch = y_batch.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(X_batch)\n",
    "                loss = self.criterion(output, y_batch)\n",
    "                epoch_loss += loss.item()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "         # Return the updated model parameters\n",
    "        if method == \"HyFDCA\":\n",
    "            return self.get_parameters(config), len(self.train_loader.dataset), {\n",
    "                \"weight_decay\": current_weight_decay,\n",
    "                \"encrypted_inner_product\": encrypted_inner_product  # Include encrypted inner product\n",
    "            }\n",
    "        else:\n",
    "            return self.get_parameters(config), len(self.train_loader.dataset), {\"weight_decay\": current_weight_decay}\n",
    "        \n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        self.model.eval()\n",
    "        loss = 0.0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in self.train_loader:\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                y_batch = y_batch.to(self.device)\n",
    "                output = self.model(X_batch)\n",
    "                loss += self.criterion(output, y_batch).item() * X_batch.size(0)\n",
    "                total += X_batch.size(0)\n",
    "        loss /= total\n",
    "        return float(loss), total, {\"MSE\": float(loss)}\n",
    "\n",
    "# Function to create client instances\n",
    "def client_fn(cid: str):\n",
    "    # Extract client ID from cid\n",
    "    client_id = int(cid)\n",
    "\n",
    "    # Validate client_id\n",
    "    if client_id >= num_clients:\n",
    "        raise ValueError(f\"Invalid client_id {client_id}. It should be less than {num_clients}.\")\n",
    "\n",
    "    # Calculate the start and end indices for this client\n",
    "    start_idx = (client_id  + random.randint(1,30))* data_per_client\n",
    "    end_idx = start_idx + data_per_client\n",
    "\n",
    "    #loop around if too big\n",
    "    if start_idx > len(train_df):\n",
    "        start_idx = start_idx - len(train_df)\n",
    "        end_idx = start_idx + data_per_client\n",
    "\n",
    "\n",
    "    # Ensure we don't exceed the dataset length\n",
    "    if end_idx > len(train_df):\n",
    "        end_idx = len(train_df)\n",
    "\n",
    "    # Slice the data for this client\n",
    "    client_data = train_df.iloc[start_idx:end_idx].reset_index(drop=True)\n",
    "\n",
    "    # Handle the case where there might be less data than expected\n",
    "    if len(client_data) < data_per_client:\n",
    "        print(f\"Client {client_id} has {len(client_data)} data points (less than expected).\")\n",
    "\n",
    "    # Debugging output\n",
    "    print(f\"Client {client_id} has {len(client_data)} training data points.\")\n",
    "\n",
    "    # Create and return a client instance with this data\n",
    "    return FLClient(client_data)\n",
    "\n",
    "# Prepare test data\n",
    "test_data_size = len(test_df)\n",
    "if test_data_size >= test_data_points:\n",
    "    test_data = test_df.iloc[:test_data_points].reset_index(drop=True)\n",
    "else:\n",
    "    raise ValueError(f\"Not enough test data. Required: {test_data_points}, Available: {test_data_size}\")\n",
    "\n",
    "# Define a function for server-side evaluation\n",
    "def get_evaluate_fn(model):\n",
    "    def evaluate(server_round, parameters, config):\n",
    "        global client_loss_data\n",
    "\n",
    "        # Update the model with the current global parameters\n",
    "        params_dict = zip(model.state_dict().keys(), parameters)\n",
    "        state_dict = {k: torch.tensor(np.copy(v), dtype=torch.float32) for k, v in params_dict}\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "        # Simulate loss evaluation for each client\n",
    "        model.eval()\n",
    "        for client_id in range(num_clients):  # Iterate over all possible clients\n",
    "            try:\n",
    "                client = client_fn(str(client_id))  # Get the client\n",
    "                client_loss = 0.0\n",
    "                total_samples = 0\n",
    "\n",
    "                # Evaluate the client's model on its own dataset\n",
    "                with torch.no_grad():\n",
    "                    for X_batch, y_batch in client.train_loader:\n",
    "                        X_batch = X_batch.to(model.device)\n",
    "                        y_batch = y_batch.to(model.device)\n",
    "                        output = model(X_batch)\n",
    "                        client_loss += nn.MSELoss()(output, y_batch).item() * X_batch.size(0)\n",
    "                        total_samples += X_batch.size(0)\n",
    "\n",
    "                # Compute the average loss for this round\n",
    "                if total_samples > 0:\n",
    "                    round_loss = client_loss / total_samples\n",
    "                else:\n",
    "                    round_loss = float(\"nan\")\n",
    "\n",
    "                # Update the cumulative loss data\n",
    "                if client_id not in client_loss_data:\n",
    "                    client_loss_data[client_id] = {\"total_loss\": 0.0, \"rounds\": 0}\n",
    "                client_loss_data[client_id][\"total_loss\"] += round_loss\n",
    "                client_loss_data[client_id][\"rounds\"] += 1\n",
    "\n",
    "            except ValueError as e:\n",
    "                # If a client does not exist (not selected), skip it\n",
    "                print(f\"Client {client_id} not included in this round: {e}\")\n",
    "\n",
    "        return None, {}  # No need to return global metrics since it's per-client loss\n",
    "    return evaluate\n",
    "\n",
    "# After the simulation, save the averaged client losses to a CSV file\n",
    "def save_averaged_device_losses(filename=\"averaged_device_losses.csv\"):\n",
    "    global client_loss_data\n",
    "\n",
    "    # Compute the average loss for each client\n",
    "    averaged_data = []\n",
    "    for client_id, data in client_loss_data.items():\n",
    "        avg_loss = data[\"total_loss\"] / data[\"rounds\"] if data[\"rounds\"] > 0 else float(\"nan\")\n",
    "        averaged_data.append({\"client_id\": client_id, \"average_loss\": avg_loss})\n",
    "\n",
    "    # Create a DataFrame and save it to CSV\n",
    "    averaged_df = pd.DataFrame(averaged_data)\n",
    "    averaged_df.to_csv(filename, index=False)\n",
    "\n",
    "# Loop for 50 iterations, changing the seed each time\n",
    "for i in range(num_iterations):\n",
    "    seed = i + 1  # Change seed in each run\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Reset global loss storage\n",
    "    client_loss_data = {}\n",
    "\n",
    "    # Initialize the global model\n",
    "    global_model = LSTMModel(\n",
    "        input_size=train_df.shape[1],\n",
    "        hidden_size=64,\n",
    "        num_layers=2,\n",
    "        prediction_length=prediction_length\n",
    "    )\n",
    "    global_model.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    global_model.to(global_model.device)\n",
    "\n",
    "    # Start simulation\n",
    "    fl.simulation.start_simulation(\n",
    "        client_fn=client_fn,\n",
    "        num_clients=num_clients,\n",
    "        config=fl.server.ServerConfig(num_rounds=num_rounds),\n",
    "        strategy=fl.server.strategy.FedAvg(\n",
    "            evaluate_fn=get_evaluate_fn(global_model)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Compute average loss for each client\n",
    "    averaged_data = {}\n",
    "    for client_id, data in client_loss_data.items():\n",
    "        avg_loss = data[\"total_loss\"] / data[\"rounds\"] if data[\"rounds\"] > 0 else float(\"nan\")\n",
    "        averaged_data[client_id] = avg_loss\n",
    "\n",
    "    # Store results in a dictionary\n",
    "    for client_id in range(num_clients):\n",
    "        if client_id not in client_loss_data_all:\n",
    "            client_loss_data_all[client_id] = []\n",
    "        client_loss_data_all[client_id].append(averaged_data.get(client_id, float(\"nan\")))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame.from_dict(client_loss_data_all, orient=\"index\")\n",
    "df.insert(0, \"client_id\", df.index)  # Insert client_id column\n",
    "df.columns = [\"client_id\"] + [f\"average_loss in {str(int(first_seed)+i)}\" for i in range(num_iterations)]  # Name columns\n",
    "\n",
    "# Save results\n",
    "output_filename = f\"ANOVA_averaged_device_losses_{method}_{dataset}.csv\"\n",
    "df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TestCuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
